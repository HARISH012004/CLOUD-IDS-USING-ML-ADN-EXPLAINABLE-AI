# CLOUD-IDS-USING-ML-ADN-EXPLAINABLE-AI
The rapid adoption of cloud computing has increased the volume of network traffic and,consequently, the exposure to sophisticated cyberattacks. Although Machine Learning (ML) and DeepLearning (DL) based Intrusion Detection Systems (IDSs) achieve high detection performance, their blackbox nature limits interpretability and reduces analyst trust in automated decisions. This work enhancesthe transparency of cloud-based IDS models through a structured Explainable Artificial Intelligence (XAI) framework. The methodology includes data preprocessing with SMOTE-based class balancing, followedby a hybrid feature selection strategy integrating Information Gain, Chi-Square, and Cuckoo Search optimization. Random Forest and XGBoost classifiers are trained with tuned hyperparameters to ensurerobust detection performance. Beyond predictive accuracy, the study systematically evaluates explanation reliability using six complementary validation metrics that assess ranking consistency, explanation stability,faithfulness, and robustness. Additionally, a tabular diffusion-based synthetic data generation approachis employed on selected CICIDS 2017 attack categories to further examine explanation behavior under controlled distributional variations. To the best of our knowledge, this is the first work in the cloud IDS domain to combine diffusion-based synthetic evaluation with comprehensive multi-metric XAI validation.A Flask-based dashboard is developed to support real-time visualization of predictions and explanations,bridging the gap between performance and interpretability
